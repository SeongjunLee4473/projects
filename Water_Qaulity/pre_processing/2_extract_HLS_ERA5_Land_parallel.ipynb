{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLS & ERA5-Land Data Extraction at Water Quality Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Description\n",
    "- This code is for extracting HLS bands and ERA5 Land data for each basin  \n",
    "  using QA band (quality flag for water) and Density-based spatial clustering of applications with noise (DBSCAN).\n",
    "\n",
    "- This code is parallelized using **Joblib**.\n",
    "  \n",
    "- The dates are based on HLS data.  \n",
    "  (In the case of South Korea, HLS observation times are around 2 AM UTC. Accordingly, ERA5-Land data for the corresponding 2 AM UTC timestamp was extracted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import importlib\n",
    "import pyproj\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from netCDF4 import Dataset\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.cluster import DBSCAN\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Set base directory\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    base_FP = '/Users/lsj'\n",
    "    cpuserver_data_FP = base_FP + '/cpuserver_data'\n",
    "    nas_data_FP = '/Volumes/qnap_nas'\n",
    "elif platform.system() == 'Linux':  # Linux systems (Workstation / CPU Server GPU Server)\n",
    "    base_FP = '/home/seongjun'\n",
    "    cpuserver_data_FP = base_FP + '/cpuserver_data' # Workstation / GPU Server\n",
    "    if not os.path.exists(cpuserver_data_FP):\n",
    "        cpuserver_data_FP = '/data' # CPU Server\n",
    "    nas_data_FP = base_FP + '/NAS'\n",
    "\n",
    "# Add Python modules path\n",
    "sys.path.append(os.path.join(base_FP, 'python_modules'))\n",
    "\n",
    "# Private modules\n",
    "import HydroAI.Data as Data\n",
    "import remote_sensing.HLS as HLS\n",
    "import Slack.slack_notifier as Slack\n",
    "importlib.reload(Data);\n",
    "importlib.reload(HLS);\n",
    "importlib.reload(Slack);\n",
    "\n",
    "# Slack\n",
    "env_path = os.path.join(base_FP, '.env')\n",
    "notipy = Slack.Notifier(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define Product and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of S30 dates: 1115\n"
     ]
    }
   ],
   "source": [
    "# Define product\n",
    "product = 'S30'\n",
    "band_list = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'qa']\n",
    "\n",
    "# Define paths\n",
    "obs_df_path = os.path.join(nas_data_FP, f'water_quality/WEIS_data/WEIS_obs_filtered_2013_2024.csv')\n",
    "obs_geojson_path = os.path.join(nas_data_FP, f'water_quality/DBSCAN/obs_water_clusters.json')\n",
    "shp_file_path = os.path.join(nas_data_FP, 'water_quality/shp_files/water_body_shp/water.shp')\n",
    "hls_dir_path = os.path.join(nas_data_FP, f'HLS/Korea/{product}')\n",
    "era5_dir_path = os.path.join(cpuserver_data_FP, 'ERA5_Land/Korea')\n",
    "save_dir_path = os.path.join(nas_data_FP, f'water_quality/final_dataset/{product}')\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Get date list\n",
    "date_list = sorted([dir for dir in os.listdir(hls_dir_path) if dir.isdigit() and len(dir) == 8])\n",
    "\n",
    "print(f\"Number of {product} dates: {len(date_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Hyperparameters for DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Hyperparameter\n",
    "eps = 0.00045\n",
    "EPS_RAD = np.radians(eps)\n",
    "MIN_SAMPLES = 5\n",
    "ALGORITHM = 'ball_tree'\n",
    "METRIC = 'haversine'\n",
    "\n",
    "# General parameters\n",
    "DISTANCE_THRESHOLD = 300 # meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. In-Situ Data (Water Qaulity Observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptNo</th>\n",
       "      <th>ptNm</th>\n",
       "      <th>obs_lat</th>\n",
       "      <th>obs_lon</th>\n",
       "      <th>obs_date</th>\n",
       "      <th>wmdep</th>\n",
       "      <th>itemTemp</th>\n",
       "      <th>itemCloa</th>\n",
       "      <th>itemTn</th>\n",
       "      <th>itemTp</th>\n",
       "      <th>itemDoc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001A05</td>\n",
       "      <td>송천1</td>\n",
       "      <td>37.658611</td>\n",
       "      <td>128.676389</td>\n",
       "      <td>3/21/13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.785</td>\n",
       "      <td>0.020</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001A05</td>\n",
       "      <td>송천1</td>\n",
       "      <td>37.658611</td>\n",
       "      <td>128.676389</td>\n",
       "      <td>4/4/13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.342</td>\n",
       "      <td>0.023</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001A05</td>\n",
       "      <td>송천1</td>\n",
       "      <td>37.658611</td>\n",
       "      <td>128.676389</td>\n",
       "      <td>5/29/13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.748</td>\n",
       "      <td>0.026</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001A05</td>\n",
       "      <td>송천1</td>\n",
       "      <td>37.658611</td>\n",
       "      <td>128.676389</td>\n",
       "      <td>6/25/13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.4</td>\n",
       "      <td>10.3</td>\n",
       "      <td>5.049</td>\n",
       "      <td>0.135</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001A05</td>\n",
       "      <td>송천1</td>\n",
       "      <td>37.658611</td>\n",
       "      <td>128.676389</td>\n",
       "      <td>7/4/13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.419</td>\n",
       "      <td>0.045</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244879</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>10/23/23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.029</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244880</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>6/18/24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244881</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>8/6/24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.014</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244882</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>9/6/24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.014</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244883</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>10/31/24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.5</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.025</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244884 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ptNo   ptNm    obs_lat     obs_lon  obs_date  wmdep  itemTemp  \\\n",
       "0       1001A05    송천1  37.658611  128.676389   3/21/13    NaN       2.5   \n",
       "1       1001A05    송천1  37.658611  128.676389    4/4/13    NaN      10.7   \n",
       "2       1001A05    송천1  37.658611  128.676389   5/29/13    NaN      17.8   \n",
       "3       1001A05    송천1  37.658611  128.676389   6/25/13    NaN      23.4   \n",
       "4       1001A05    송천1  37.658611  128.676389    7/4/13    NaN      20.6   \n",
       "...         ...    ...        ...         ...       ...    ...       ...   \n",
       "244879  5303D70  수락저수지  34.846111  126.265000  10/23/23    NaN      19.2   \n",
       "244880  5303D70  수락저수지  34.846111  126.265000   6/18/24    NaN      26.2   \n",
       "244881  5303D70  수락저수지  34.846111  126.265000    8/6/24    NaN      31.7   \n",
       "244882  5303D70  수락저수지  34.846111  126.265000    9/6/24    NaN      30.0   \n",
       "244883  5303D70  수락저수지  34.846111  126.265000  10/31/24    NaN      18.5   \n",
       "\n",
       "        itemCloa  itemTn  itemTp  itemDoc  \n",
       "0            1.5   3.785   0.020     10.5  \n",
       "1            2.8   3.342   0.023     12.6  \n",
       "2            5.5   2.748   0.026     10.2  \n",
       "3           10.3   5.049   0.135      9.4  \n",
       "4            3.9   4.419   0.045      9.7  \n",
       "...          ...     ...     ...      ...  \n",
       "244879      16.1   0.695   0.029      7.5  \n",
       "244880       4.4   0.304   0.010      9.5  \n",
       "244881      10.5   0.558   0.014     10.1  \n",
       "244882       6.8   0.270   0.014      8.8  \n",
       "244883      13.2   0.353   0.025      8.9  \n",
       "\n",
       "[244884 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_df = pd.read_csv(obs_df_path)\n",
    "required_columns = ['latitude', 'longitude', 'wmcymd']\n",
    "if all(col in obs_df.columns for col in required_columns):\n",
    "    obs_df.rename(columns={'latitude': 'obs_lat', 'longitude': 'obs_lon', 'wmcymd': 'obs_date'}, inplace=True)\n",
    "obs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract HLS and ERA5 Land data for each date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning !\n",
    "* This code assumes that all HLS bands have the **same shape**.\n",
    "* If the HLS bands have different shapes, you need to adjust the code accordingly.\n",
    "* Please check the shape of the HLS bands before running the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_date(date, obs_df, hls_dir_path, era5_dir_path, save_dir_path, product, band_list):\n",
    "    \"\"\"\n",
    "    Processes all data for a single date and saves the result to a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Checkpoint: skip if the file already exists\n",
    "        daily_output_path = os.path.join(save_dir_path, f'{product}_{date}.feather')\n",
    "        if os.path.exists(daily_output_path):\n",
    "            return f\"Skipping {date} because it already exists\"\n",
    "\n",
    "        # =============================================================================#\n",
    "        # HLS pre-processing\n",
    "        # =============================================================================#\n",
    "\n",
    "        # 1. Temporal filtering\n",
    "        # 1.1. Integrate date type\n",
    "        obs_df_filtered = obs_df.copy()\n",
    "        obs_df_filtered['obs_date'] = pd.to_datetime(obs_df_filtered['obs_date'], format='%m/%d/%y')\n",
    "        hls_pd_date = pd.to_datetime(date, format='%Y%m%d')\n",
    "\n",
    "        # 1.2. Temporal filtering (1 day window)\n",
    "        time_window = timedelta(days=1)\n",
    "        date_mask = (obs_df_filtered['obs_date'] >= hls_pd_date - time_window) & \\\n",
    "                    (obs_df_filtered['obs_date'] <= hls_pd_date + time_window)\n",
    "\n",
    "        obs_df_filtered = obs_df_filtered[date_mask].copy()\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 2. Load HLS data\n",
    "        # 2.1 Read HLS data\n",
    "        hls_path = os.path.join(hls_dir_path, date)\n",
    "        HLS_reader = HLS.BandReader(hls_path, product, date)\n",
    "        _, transform, _ = HLS_reader.get_band_with_transform(band_list[0], product)  # Get for the first band (load transform)\n",
    "\n",
    "        # 2.2. Read entire HLS bands\n",
    "        NO_DATA_VALUE = -0.9999 # (-9999 * scale factor)\n",
    "        band_data = {}\n",
    "        for band in band_list:\n",
    "            band_data[band] = getattr(HLS_reader, band)\n",
    "            band_data[band][band_data[band] == NO_DATA_VALUE] = np.nan\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 3. Masking noisy pixels\n",
    "        # 3.1. Create a noise mask from QA band\n",
    "        # QA band bit number:\n",
    "        # 0 = Cirrus\n",
    "        # 1 = Cloud\n",
    "        # 2 = Adjacent to cloud/shadow\n",
    "        # 3 = Cloud shadow\n",
    "        # 4 = Snow/ice\n",
    "        # 5 = Water\n",
    "        # 6-7 = aerosol optical thickness (AOT) level\n",
    "\n",
    "        # 96 (01100000) = Water, low confidence\n",
    "        # 160 (10100000) = Water, medium confidence\n",
    "        # 224 (11100000) = Water, high confidence\n",
    "        # We create a 'noise_mask' that is True for everything EXCEPT water.\n",
    "\n",
    "        water_mask = np.isin(band_data['qa'], [96, 160, 224])\n",
    "        noise_mask = ~np.isin(band_data['qa'], [96, 160, 224])\n",
    "\n",
    "        # 3.2. Apply the mask to all bands\n",
    "        # This sets the noisy pixels in all bands to NaN (Not a Number)\n",
    "        for band in band_list:\n",
    "            if band == 'qa': # Skip QA band (integer)\n",
    "                continue\n",
    "            band_data[band] = band_data[band].astype(np.float32) # Convert to float to allow NaN\n",
    "            band_data[band][noise_mask] = np.nan\n",
    "\n",
    "        # 3.3. Clip negative values to 0 for all bands\n",
    "        # This ensures all pixel values are non-negative\n",
    "        for band in band_list:\n",
    "            if band != 'qa':\n",
    "                band_data[band] = np.clip(band_data[band], 0, None)\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 4. Calculate MNDWI for each observation point\n",
    "        # 4.1. Get all observation coordinates from the DataFrame\n",
    "        lons = obs_df_filtered['obs_lon'].values\n",
    "        lats = obs_df_filtered['obs_lat'].values\n",
    "\n",
    "        # 4.2. Convert all geographic coordinates to pixel coordinates (row, col) at once\n",
    "        rows, cols = rowcol(transform, lons, lats)\n",
    "        rows, cols = np.array(rows), np.array(cols)\n",
    "\n",
    "        # 4.3. Create a mask to filter out points that are outside the image boundaries\n",
    "        max_row, max_col = band_data['green'].shape\n",
    "        valid_coords_mask = (rows >= 0) & (rows < max_row) & (cols >= 0) & (cols < max_col)\n",
    "\n",
    "        # 4.4. Filter out points that are outside the image boundaries\n",
    "        is_water_at_point = np.zeros(len(obs_df_filtered), dtype=bool)\n",
    "        valid_rows = rows[valid_coords_mask]\n",
    "        valid_cols = cols[valid_coords_mask]\n",
    "\n",
    "        is_water_values = ~np.isnan(band_data['green'][valid_rows, valid_cols])\n",
    "        is_water_at_point[valid_coords_mask] = is_water_values\n",
    "\n",
    "        obs_noise_filtered_df = obs_df_filtered[is_water_at_point].reset_index(drop=True)\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 5. DBSCAN Clustering\n",
    "        # 5.1. Get the pixel coordinates (row, col) of all water pixels\n",
    "        water_pixel_indices = np.argwhere(water_mask)\n",
    "\n",
    "        if water_pixel_indices.size == 0:\n",
    "            water_labels = np.array([])\n",
    "            water_gdf = gpd.GeoDataFrame({'cluster_label': [], 'geometry': []}, crs=\"EPSG:4326\")\n",
    "        else:\n",
    "            rows = water_pixel_indices[:, 0]\n",
    "            cols = water_pixel_indices[:, 1]\n",
    "\n",
    "            # 5.1.1. Convert pixel coordinates to geographic coordinates (lon, lat)\n",
    "            water_lons, water_lats = rasterio.transform.xy(transform, rows, cols)\n",
    "\n",
    "            # 5.1.2. Convert to radians for Haversine distance\n",
    "            coords_rad = np.vstack((np.radians(water_lats), np.radians(water_lons))).T\n",
    "\n",
    "            # 5.1.3. Perform DBSCAN clustering\n",
    "            # Note: You might need to adjust EPS and MIN_SAMPLES for this new approach\n",
    "            db = DBSCAN(eps=EPS_RAD,\n",
    "                        min_samples=MIN_SAMPLES,\n",
    "                        algorithm=ALGORITHM,\n",
    "                        metric=METRIC,\n",
    "                        n_jobs=-1\n",
    "                        ).fit(coords_rad)\n",
    "            water_labels = db.labels_\n",
    "\n",
    "            # 5.1.4. Create a GeoDataFrame for all clustered water pixels\n",
    "            water_gdf = gpd.GeoDataFrame({\n",
    "                'cluster_label': water_labels,\n",
    "                'geometry': gpd.points_from_xy(water_lons, water_lats)\n",
    "            }, crs=\"EPSG:4326\")\n",
    "\n",
    "            water_gdf = water_gdf[water_gdf['cluster_label'] != -1]\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 6. GeoDataFrame for Obs & HLS\n",
    "        # 6.1. Create a GeoDataFrame for the observation stations from obs_df_filtered\n",
    "        stations_gdf = gpd.GeoDataFrame(\n",
    "            obs_noise_filtered_df,\n",
    "            geometry=gpd.points_from_xy(obs_noise_filtered_df['obs_lon'], obs_noise_filtered_df['obs_lat']),\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "\n",
    "        # 6.2. Define the projected CRS for Korea (in meters)\n",
    "        PROJECTED_CRS = \"EPSG:5186\" # UTM-K\n",
    "\n",
    "        # 6.3. Re-project both GeoDataFrames to the projected CRS\n",
    "        stations_gdf_proj = stations_gdf.to_crs(PROJECTED_CRS)\n",
    "        water_gdf_proj = water_gdf.to_crs(PROJECTED_CRS)\n",
    "\n",
    "        # 6.4. Use sjoin_nearest on the re-projected data\n",
    "        # Now, 'max_distance' can be set directly in meters.\n",
    "        final_matchup_gdf_proj = gpd.sjoin_nearest(\n",
    "            stations_gdf_proj,\n",
    "            water_gdf_proj,\n",
    "            how='left',\n",
    "            max_distance=DISTANCE_THRESHOLD # Use the meter-based threshold directly\n",
    "        )\n",
    "\n",
    "        # 6.5. Convert the result back to the original CRS (WGS84) for consistency\n",
    "        final_matchup_gdf = final_matchup_gdf_proj.to_crs(stations_gdf.crs)\n",
    "\n",
    "        # 6.6. Insert HLS date\n",
    "        final_matchup_gdf['hls_date'] = pd.to_datetime(date, format='%Y%m%d')\n",
    "        final_matchup_gdf.insert(5, 'hls_date', final_matchup_gdf.pop('hls_date'))\n",
    "\n",
    "        # 6.7. Drop the extra index column created by the join\n",
    "        final_matchup_gdf = final_matchup_gdf.drop(columns=['index_right'])\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 7. Extract band values for all water pixels\n",
    "        # 7.1. Convert longitude/latitude to pixel coordinates (row, col)\n",
    "        water_lons = water_gdf.geometry.x\n",
    "        water_lats = water_gdf.geometry.y\n",
    "        rows, cols = rasterio.transform.rowcol(transform, water_lons, water_lats)\n",
    "\n",
    "        # 7.2. Add pixel values of each band to water_gdf as new columns\n",
    "        for band in band_list:\n",
    "            if band != 'qa':\n",
    "                water_gdf[band] = band_data[band][rows, cols]\n",
    "\n",
    "        # 7.3. Calculate statistics for each cluster\n",
    "        # 7.3.1. Define the statistics to calculate\n",
    "        # Example: {'green': ['mean', 'median', 'std'], 'red': ['mean', 'median', 'std'], ...}\n",
    "        aggregations = {band: ['mean', 'median', 'std'] for band in band_list if band != 'qa'}\n",
    "\n",
    "        # 7.3.2. Calculate statistics for each cluster\n",
    "        cluster_stats = water_gdf.groupby('cluster_label').agg(aggregations)\n",
    "\n",
    "        # 7.3.3. Change the column names of the calculated statistics\n",
    "        # Example: ('green', 'median') -> 'green', ('green', 'std') -> 'green_std'\n",
    "        cluster_stats.columns = ['_'.join(col).strip() for col in cluster_stats.columns.values]\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 8. Merge statistics into final_matchup_gdf\n",
    "        # 8.1. Merge on 'cluster_label'\n",
    "        obs_hls_df = pd.merge(\n",
    "            final_matchup_gdf,\n",
    "            cluster_stats,\n",
    "            on='cluster_label',\n",
    "            how='left' # Keep all rows from final_matchup_gdf\n",
    "        )\n",
    "\n",
    "        # 8.2. Add 'sensor' column at the 5th position (index 5)\n",
    "        obs_hls_df.insert(5, 'sensor', product)\n",
    "        final_obs_hls_df = obs_hls_df.copy()\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 9. Calculate satellite-based indices\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            # 1) NDWI\n",
    "            final_obs_hls_df['NDWI'] = (final_obs_hls_df['nir_mean'] - final_obs_hls_df['swir1_mean']) / (final_obs_hls_df['nir_mean'] + final_obs_hls_df['swir1_mean'])\n",
    "            # 2) MNDWI\n",
    "            final_obs_hls_df['MNDWI'] = (final_obs_hls_df['green_mean'] - final_obs_hls_df['swir1_mean']) / (final_obs_hls_df['green_mean'] + final_obs_hls_df['swir1_mean'])\n",
    "            # 3) NDVI (for water)\n",
    "            final_obs_hls_df['NDVI'] = (final_obs_hls_df['nir_mean'] - final_obs_hls_df['red_mean']) / (final_obs_hls_df['nir_mean'] + final_obs_hls_df['red_mean'])\n",
    "            # 4) Blue-Green Ratio\n",
    "            final_obs_hls_df['Green_Blue_Ratio'] = final_obs_hls_df['green_mean'] / final_obs_hls_df['blue_mean']\n",
    "            # 5) NIR-Red Ratio\n",
    "            final_obs_hls_df['NIR_Red_Ratio'] = final_obs_hls_df['nir_mean'] / final_obs_hls_df['red_mean']\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 10. Replace inf and -inf with NaN\n",
    "        final_obs_hls_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # =============================================================================#\n",
    "        # ERA5 pre-processing\n",
    "        # =============================================================================#\n",
    "\n",
    "        # 1. Load ERA5 grid coordinates and data\n",
    "        year = date[:4]\n",
    "        era5_path = os.path.join(era5_dir_path, f'{year}/ERA5_land_{date}.nc')\n",
    "        ds = Dataset(era5_path, 'r')\n",
    "\n",
    "        ##----------------------------------------------------------------------------##\n",
    "\n",
    "        # 2. Pre-process ERA5 grid coordinates\n",
    "        # 2.1. Convert WGS84 (EPSG:4326) to UTM-K (EPSG:5186)\n",
    "        transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", \"EPSG:5186\", always_xy=True)\n",
    "\n",
    "        lon_era5 = ds.variables['longitude'][:]\n",
    "        lat_era5 = ds.variables['latitude'][:]\n",
    "        lon_2d, lat_2d = np.meshgrid(lon_era5, lat_era5)\n",
    "\n",
    "        # 2.2. Convert ERA5 grid coordinates to UTM-K (EPSG:5186)\n",
    "        era5_x, era5_y = transformer.transform(lon_2d.ravel(), lat_2d.ravel())\n",
    "        grid_points_proj = np.vstack([era5_x, era5_y]).T\n",
    "        era5_tree = cKDTree(grid_points_proj)\n",
    "\n",
    "        # 2.3. Convert observation coordinates to UTM-K (EPSG:5186)\n",
    "        station_lon = final_obs_hls_df['obs_lon'].values\n",
    "        station_lat = final_obs_hls_df['obs_lat'].values\n",
    "        station_x, station_y = transformer.transform(station_lon, station_lat)\n",
    "        station_points_proj = np.vstack([station_x, station_y]).T\n",
    "\n",
    "        # 2.4. Find the index of the closest ERA5 grid point for each observation\n",
    "        _, indices = era5_tree.query(station_points_proj, k=1)\n",
    "        lat_indices, lon_indices = np.unravel_index(indices, lon_2d.shape)\n",
    "\n",
    "        # 3. Pre-process precipitation data (tp)\n",
    "        # 3.1. Calculate the date for the day before the target date\n",
    "        target_date_dt = datetime.strptime(date, '%Y%m%d')\n",
    "        era5_path_1 = os.path.join(era5_dir_path, f'{year}/ERA5_land_{(target_date_dt - timedelta(days=1)).strftime(\"%Y%m%d\")}.nc')\n",
    "        era5_path_2 = os.path.join(era5_dir_path, f'{year}/ERA5_land_{(target_date_dt - timedelta(days=2)).strftime(\"%Y%m%d\")}.nc')\n",
    "\n",
    "        # 3.2. Load the data for the previous days\n",
    "        with Dataset(era5_path_1, 'r') as ds1:\n",
    "            tp_day_1 = ds1.variables['tp'][:]\n",
    "        with Dataset(era5_path_2, 'r') as ds2:\n",
    "            tp_day_2 = ds2.variables['tp'][:]\n",
    "\n",
    "        tp_day_0 = ds.variables['tp'][:]\n",
    "\n",
    "        # 3.3. Calculate the cumulative precipitation and add it to final_obs_hls_df\n",
    "        tp_24hr = np.sum(tp_day_0[:, lat_indices, lon_indices], axis=0)\n",
    "        tp_48hr = tp_24hr + np.sum(tp_day_1[:, lat_indices, lon_indices], axis=0)\n",
    "        tp_72hr = tp_48hr + np.sum(tp_day_2[:, lat_indices, lon_indices], axis=0)\n",
    "\n",
    "        final_obs_hls_df['tp_24h'] = tp_24hr\n",
    "        final_obs_hls_df['tp_48h'] = tp_48hr\n",
    "        final_obs_hls_df['tp_72h'] = tp_72hr\n",
    "\n",
    "        # 4. Pre-process other variables\n",
    "        exclude_vars = {\"number\", \"valid_time\", \"expver\", \"latitude\", \"longitude\",\n",
    "                        \"stl2\", \"stl3\", \"stl4\", \"asn\", \"snowc\", \"rsn\", \"lict\",\n",
    "                        \"sde\", \"sd\", \"swvl2\", \"swvl3\", \"swvl4\", \"ssrd\", \"strd\", 'lai_hv', 'lai_lv'}\n",
    "\n",
    "        for var_name in ds.variables:\n",
    "            if var_name not in exclude_vars and var_name in ds.variables:\n",
    "                # Calculate the daily mean value\n",
    "                var_data = ds.variables[var_name][:]\n",
    "                values = var_data[2, lat_indices, lon_indices]\n",
    "                final_obs_hls_df[var_name] = values\n",
    "\n",
    "        ds.close()\n",
    "\n",
    "        # 5. Save the result\n",
    "        if not final_obs_hls_df.empty:\n",
    "            final_obs_hls_df.to_feather(daily_output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed to process {date}: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Parallel Processing for Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel processing for 1115 dates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend LokyBackend with 32 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done   8 tasks      | elapsed:  1.7min\n",
      "/home/seongjun/miniconda3/envs/lsj/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=32)]: Done  21 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=32)]: Done  34 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=32)]: Done  49 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=32)]: Done  64 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=32)]: Done  81 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=32)]: Done  98 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=32)]: Done 117 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed: 25.1min\n",
      "[Parallel(n_jobs=32)]: Done 157 tasks      | elapsed: 32.1min\n",
      "[Parallel(n_jobs=32)]: Done 178 tasks      | elapsed: 37.2min\n",
      "[Parallel(n_jobs=32)]: Done 201 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=32)]: Done 224 tasks      | elapsed: 50.4min\n",
      "[Parallel(n_jobs=32)]: Done 249 tasks      | elapsed: 54.3min\n",
      "[Parallel(n_jobs=32)]: Done 274 tasks      | elapsed: 63.4min\n",
      "[Parallel(n_jobs=32)]: Done 301 tasks      | elapsed: 71.6min\n",
      "[Parallel(n_jobs=32)]: Done 328 tasks      | elapsed: 78.8min\n",
      "[Parallel(n_jobs=32)]: Done 357 tasks      | elapsed: 85.3min\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed: 89.6min\n",
      "[Parallel(n_jobs=32)]: Done 417 tasks      | elapsed: 99.1min\n",
      "[Parallel(n_jobs=32)]: Done 448 tasks      | elapsed: 107.9min\n",
      "[Parallel(n_jobs=32)]: Done 481 tasks      | elapsed: 117.9min\n",
      "[Parallel(n_jobs=32)]: Done 514 tasks      | elapsed: 123.5min\n",
      "[Parallel(n_jobs=32)]: Done 549 tasks      | elapsed: 135.0min\n",
      "[Parallel(n_jobs=32)]: Done 584 tasks      | elapsed: 144.2min\n",
      "[Parallel(n_jobs=32)]: Done 621 tasks      | elapsed: 152.4min\n",
      "[Parallel(n_jobs=32)]: Done 658 tasks      | elapsed: 160.1min\n",
      "[Parallel(n_jobs=32)]: Done 697 tasks      | elapsed: 172.6min\n",
      "[Parallel(n_jobs=32)]: Done 736 tasks      | elapsed: 185.5min\n",
      "[Parallel(n_jobs=32)]: Done 777 tasks      | elapsed: 193.3min\n",
      "[Parallel(n_jobs=32)]: Done 818 tasks      | elapsed: 204.8min\n",
      "[Parallel(n_jobs=32)]: Done 861 tasks      | elapsed: 218.3min\n",
      "[Parallel(n_jobs=32)]: Done 904 tasks      | elapsed: 226.7min\n",
      "[Parallel(n_jobs=32)]: Done 949 tasks      | elapsed: 237.7min\n",
      "[Parallel(n_jobs=32)]: Done 994 tasks      | elapsed: 249.7min\n",
      "[Parallel(n_jobs=32)]: Done 1041 tasks      | elapsed: 259.8min\n",
      "[Parallel(n_jobs=32)]: Done 1115 out of 1115 | elapsed: 285.4min finished\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# Parallel processing for HLS and ERA5 Land data\n",
    "# =================================================================================\n",
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        # --- 1. Global variables and path settings ---\n",
    "        # obs_df, date_list, hls_dir_path, era5_dir_path 등...\n",
    "\n",
    "        # --- 2. Parallel execution ---\n",
    "        # n_jobs=-1 means using all CPU cores. Adjust as needed to avoid server overload (e.g., 180)\n",
    "        print(f\"Starting parallel processing for {len(date_list)} dates...\")\n",
    "\n",
    "        results = Parallel(n_jobs=32, verbose=10)(\n",
    "            delayed(process_single_date)(\n",
    "                date, obs_df, hls_dir_path, era5_dir_path, save_dir_path, product, band_list\n",
    "            ) for date in date_list\n",
    "        )\n",
    "        notipy.send_success()\n",
    "\n",
    "except Exception as e:\n",
    "    notipy.send_error(e)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Merge `.feather` to `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'S30'\n",
    "save_dir_path = f'/home/seongjun/NAS/water_quality/final_dataset/{product}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating all daily result files...\n",
      "\n",
      "All 648 files have been consolidated into '/home/seongjun/NAS/water_quality/final_dataset/L30/L30_obs_era5_land.csv'\n"
     ]
    }
   ],
   "source": [
    "print(\"Consolidating all daily result files...\")\n",
    "file_list = [os.path.join(save_dir_path, f) for f in os.listdir(save_dir_path) if f.endswith('.feather')]\n",
    "if file_list:\n",
    "    all_dates_df = pd.concat([pd.read_feather(f) for f in file_list], ignore_index=True)\n",
    "    all_dates_df = all_dates_df.drop(columns=['geometry', 'cluster_label'])\n",
    "    all_dates_df = all_dates_df.sort_values(by=['ptNo', 'obs_date'])\n",
    "    file_path = os.path.join(save_dir_path, f'{product}_obs_era5_land.csv')\n",
    "    all_dates_df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nAll {len(file_list)} files have been consolidated into '{file_path}'\")\n",
    "else:\n",
    "    print(\"No result files found to consolidate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptNo</th>\n",
       "      <th>ptNm</th>\n",
       "      <th>obs_lat</th>\n",
       "      <th>obs_lon</th>\n",
       "      <th>obs_date</th>\n",
       "      <th>sensor</th>\n",
       "      <th>hls_date</th>\n",
       "      <th>wmdep</th>\n",
       "      <th>itemTemp</th>\n",
       "      <th>itemCloa</th>\n",
       "      <th>...</th>\n",
       "      <th>pev</th>\n",
       "      <th>ro</th>\n",
       "      <th>es</th>\n",
       "      <th>ssro</th>\n",
       "      <th>sro</th>\n",
       "      <th>e</th>\n",
       "      <th>u10</th>\n",
       "      <th>v10</th>\n",
       "      <th>sp</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001A30</td>\n",
       "      <td>조양강</td>\n",
       "      <td>37.439167</td>\n",
       "      <td>128.652778</td>\n",
       "      <td>2013-09-03</td>\n",
       "      <td>L30</td>\n",
       "      <td>2013-09-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>4.365575e-11</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.564622e-07</td>\n",
       "      <td>-0.000723</td>\n",
       "      <td>0.765411</td>\n",
       "      <td>-0.643417</td>\n",
       "      <td>94457.56</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001A30</td>\n",
       "      <td>조양강</td>\n",
       "      <td>37.439167</td>\n",
       "      <td>128.652778</td>\n",
       "      <td>2014-10-15</td>\n",
       "      <td>L30</td>\n",
       "      <td>2014-10-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-1.446213</td>\n",
       "      <td>-1.602402</td>\n",
       "      <td>94594.50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001A30</td>\n",
       "      <td>조양강</td>\n",
       "      <td>37.439167</td>\n",
       "      <td>128.652778</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>L30</td>\n",
       "      <td>2015-07-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001295</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1.455191e-11</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>5.960465e-08</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>-0.911423</td>\n",
       "      <td>-0.578705</td>\n",
       "      <td>94273.06</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001A30</td>\n",
       "      <td>조양강</td>\n",
       "      <td>37.439167</td>\n",
       "      <td>128.652778</td>\n",
       "      <td>2015-09-09</td>\n",
       "      <td>L30</td>\n",
       "      <td>2015-09-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001488</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.365575e-11</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>-2.955643</td>\n",
       "      <td>-2.237575</td>\n",
       "      <td>94735.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001A30</td>\n",
       "      <td>조양강</td>\n",
       "      <td>37.439167</td>\n",
       "      <td>128.652778</td>\n",
       "      <td>2016-12-07</td>\n",
       "      <td>L30</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000516</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-4.924717e-06</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>5.215406e-08</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>1.942169</td>\n",
       "      <td>0.243530</td>\n",
       "      <td>95017.31</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15477</th>\n",
       "      <td>5303D60</td>\n",
       "      <td>원산저수지</td>\n",
       "      <td>34.790833</td>\n",
       "      <td>126.110556</td>\n",
       "      <td>2024-06-18</td>\n",
       "      <td>L30</td>\n",
       "      <td>2024-06-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15478</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>L30</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.5</td>\n",
       "      <td>11.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15479</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>L30</td>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15480</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>2024-06-18</td>\n",
       "      <td>L30</td>\n",
       "      <td>2024-06-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15481</th>\n",
       "      <td>5303D70</td>\n",
       "      <td>수락저수지</td>\n",
       "      <td>34.846111</td>\n",
       "      <td>126.265000</td>\n",
       "      <td>2024-08-06</td>\n",
       "      <td>L30</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15482 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ptNo   ptNm    obs_lat     obs_lon    obs_date sensor    hls_date  \\\n",
       "0      1001A30    조양강  37.439167  128.652778  2013-09-03    L30  2013-09-02   \n",
       "1      1001A30    조양강  37.439167  128.652778  2014-10-15    L30  2014-10-14   \n",
       "2      1001A30    조양강  37.439167  128.652778  2015-07-07    L30  2015-07-06   \n",
       "3      1001A30    조양강  37.439167  128.652778  2015-09-09    L30  2015-09-08   \n",
       "4      1001A30    조양강  37.439167  128.652778  2016-12-07    L30  2016-12-06   \n",
       "...        ...    ...        ...         ...         ...    ...         ...   \n",
       "15477  5303D60  원산저수지  34.790833  126.110556  2024-06-18    L30  2024-06-18   \n",
       "15478  5303D70  수락저수지  34.846111  126.265000  2019-06-13    L30  2019-06-13   \n",
       "15479  5303D70  수락저수지  34.846111  126.265000  2023-08-04    L30  2023-08-03   \n",
       "15480  5303D70  수락저수지  34.846111  126.265000  2024-06-18    L30  2024-06-18   \n",
       "15481  5303D70  수락저수지  34.846111  126.265000  2024-08-06    L30  2024-08-05   \n",
       "\n",
       "       wmdep  itemTemp  itemCloa  ...       pev        ro            es  \\\n",
       "0        NaN      22.5       6.5  ... -0.001171  0.000078  4.365575e-11   \n",
       "1        NaN      13.0       1.7  ... -0.000919  0.000067  0.000000e+00   \n",
       "2        NaN      22.1       8.2  ... -0.001295  0.000022  1.455191e-11   \n",
       "3        NaN      18.7       2.5  ... -0.001488  0.000011  4.365575e-11   \n",
       "4        NaN       2.9       0.5  ... -0.000516  0.000041 -4.924717e-06   \n",
       "...      ...       ...       ...  ...       ...       ...           ...   \n",
       "15477    NaN      21.9       0.4  ...       NaN       NaN           NaN   \n",
       "15478    NaN      20.5      11.4  ...       NaN       NaN           NaN   \n",
       "15479    NaN      23.0       8.6  ...       NaN       NaN           NaN   \n",
       "15480    NaN      26.2       4.4  ...       NaN       NaN           NaN   \n",
       "15481    NaN      31.7      10.5  ...       NaN       NaN           NaN   \n",
       "\n",
       "           ssro           sro         e       u10       v10        sp  \\\n",
       "0      0.000078  1.564622e-07 -0.000723  0.765411 -0.643417  94457.56   \n",
       "1      0.000067  0.000000e+00 -0.000337 -1.446213 -1.602402  94594.50   \n",
       "2      0.000022  5.960465e-08 -0.000769 -0.911423 -0.578705  94273.06   \n",
       "3      0.000011  0.000000e+00 -0.000699 -2.955643 -2.237575  94735.00   \n",
       "4      0.000041  5.215406e-08 -0.000059  1.942169  0.243530  95017.31   \n",
       "...         ...           ...       ...       ...       ...       ...   \n",
       "15477       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "15478       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "15479       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "15480       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "15481       NaN           NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "             tp  \n",
       "0      0.000020  \n",
       "1      0.000000  \n",
       "2      0.000007  \n",
       "3      0.000000  \n",
       "4      0.000000  \n",
       "...         ...  \n",
       "15477       NaN  \n",
       "15478       NaN  \n",
       "15479       NaN  \n",
       "15480       NaN  \n",
       "15481       NaN  \n",
       "\n",
       "[15482 rows x 73 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read final dataset\n",
    "final_dataset = pd.read_csv(os.path.join(save_dir_path, f'{product}_obs_era5_land.csv'))\n",
    "final_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsj",
   "language": "python",
   "name": "lsj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
